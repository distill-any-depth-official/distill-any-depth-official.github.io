<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator">
  <meta name="keywords" content="Monocular Depth Estimation, Distillation, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/shuiyued" target="_blank">Xiankang He</a><sup>1</sup>,
              </span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Dongyan Guo</a><sup>1</sup>,
                </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hongji Li</a><sup>2</sup>,
                  </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ruibo Li</a><sup>3</sup>,
                  </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ying Cui</a><sup>1</sup>,
                  </span>
              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Chi Zhang</a><sup>*</sup>
                  </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ZJUT&emsp;&emsp;&emsp;<sup>2</sup>LZU&emsp;&emsp;&emsp;<sup>3</sup>NTU&emsp;&emsp;&emsp;<sup>4</sup>WestLake University </span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates corresponding author</small></span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.08503"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"> 
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/shuiyued/DistillAnyDepth" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Demo link -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/xingyang1/Distill-Any-Depth" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf.png" alt="Hugging Face Demo">
                    </span>
                    <span>Demo</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="assets/teaser.jpg" alt=""/>

        <h2 class="content has-text-centered">
            <strong>Zero-shot prediction on in-the-wild images.</strong> Our model, distilled from Genpercept and DepthAnythingv2, outperforms other methods by delivering more accurate depth details and exhibiting superior generalization for monocular depth estimation on in-the-wild images. 
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->

  
  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-2">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Monocular depth estimation (MDE) aims to predict scene depth from a single RGB image and plays a crucial role in 3D scene understanding. Recent advances in zero-shot MDE leverage normalized depth representations and distillation-based learning to improve generalization across diverse scenes. However, current depth normalization methods for distillation, relying on global normalization, can amplify noisy pseudo-labels, reducing distillation effectiveness. In this paper, we systematically analyze the impact of different depth normalization strategies on pseudo-label distillation. Based on our findings, we propose Cross-Context Distillation, which integrates global and local depth cues to enhance pseudo-label quality. Additionally, we introduce a multi-teacher distillation framework that leverages complementary strengths of different depth estimation models, leading to more robust and accurate depth predictions. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, both quantitatively and qualitatively.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->
  

  <!-- Paper poster -->
  <section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-">
            <h2 class="title is-2">Method</h2>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                  Our solution is designed to be <b>fine-tuning free</b> and can be combined with different methods.
                </p>
              </td>
            </div>

            <div class="columns is-centered has-text-justified">
              <td colspan="3">
                <img src="assets/crossModalAdaIN.jpg" alt="" width="700" />
              </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                  <b>Cross-Modal AdaIN</b>: We propose a novel method for depth estimation that better integrates both text and image conditioning.
                </p>
              </td>
            </div>
            <p><br></p>
            <div class="columns is-centered has-text-justified">
            <td colspan="3">
                <img src="assets/teacherModel.jpg" alt="" width="700" />
            </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                    <b>Teacher Model</b>: We propose a method that stabilizes the layout during denoising by using a teacher model to guide the transfer of style.
                </p>
              </td>
            </div>
            <p><br></p>

            <div class="columns is-centered has-text-justified">
            <td colspan="4">
                <img src="assets/SCFG.jpg" alt="" width="300" />
            </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                    <b>Style-Based CFG</b>:  We utilize layout-controlled generation, which helps in preserving the structure while removing undesired styles.
                </p>
              </td>
            </div>
          </div>
  </section>
  <!--End paper poster -->


  <!-- BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{yan2024distillanydepth,
        title={DistillAnyDepth: Refined Distillation Creates a Stronger Monocular Depth Estimator}, 
        author={Yanyan and Qi Wu},
        year={2024},
        eprint={2412.08503},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2412.08503}, 
      }</code></pre>
    </div>
  </section>
  <!--End BibTeX citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Website source code based on the <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
