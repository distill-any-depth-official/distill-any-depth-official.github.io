<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator">
  <meta name="keywords" content="Monocular Depth Estimation, Distillation, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/shuiyued" target="_blank">Xiankang He</a><sup>1</sup>,
              </span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Dongyan Guo</a><sup>1</sup>,
                </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hongji Li</a><sup>2</sup>,
                  </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ruibo Li</a><sup>3</sup>,
                  </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ying Cui</a><sup>1</sup>,
                  </span>
              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Chi Zhang</a><sup>4*</sup>
                  </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ZJUT&emsp;&emsp;&emsp;<sup>2</sup>LZU&emsp;&emsp;&emsp;<sup>3</sup>NTU&emsp;&emsp;&emsp;<sup>4</sup>WestLake University </span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates corresponding author</small></span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.08503"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"> 
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/shuiyued/DistillAnyDepth" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Demo link -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/xingyang1/Distill-Any-Depth" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf.png" alt="Hugging Face Demo">
                    </span>
                    <span>Demo</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="images/teaser8_00.png" alt=""/>

        <h2 class="content has-text-centered">
            <strong>Zero-shot prediction on in-the-wild images.</strong> Our model, distilled from Genpercept and DepthAnythingv2, outperforms other methods by delivering more accurate depth details and exhibiting superior generalization for monocular depth estimation on in-the-wild images. 
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->

  
  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-2">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Monocular depth estimation (MDE) aims to predict scene depth from a single RGB image and plays a crucial role in 3D scene understanding. Recent advances in zero-shot MDE leverage normalized depth representations and distillation-based learning to improve generalization across diverse scenes. However, current depth normalization methods for distillation, relying on global normalization, can amplify noisy pseudo-labels, reducing distillation effectiveness. In this paper, we systematically analyze the impact of different depth normalization strategies on pseudo-label distillation. Based on our findings, we propose Cross-Context Distillation, which integrates global and local depth cues to enhance pseudo-label quality. Additionally, we introduce a multi-teacher distillation framework that leverages complementary strengths of different depth estimation models, leading to more robust and accurate depth predictions. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, both quantitatively and qualitatively.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->
  

  <!-- Paper poster -->
  <section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-">
            <h2 class="title is-2">Method</h2>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                  <strong>{Overview of Cross-Context Distillation.</strong> Our method combines local and global depth information to enhance the student model’s predictions. It includes two scenarios: (1) \textit{Shared-Context Distillation}, where both models use the same image for distillation; and (2) \textit{Local-Global Distillation}, where the teacher predicts depth for overlapping patches while the student predicts the full image. The Local-Global loss $\mathcal{L}_{\text{lg}}$ (Top Right) ensures consistency between local and global predictions, enabling the student to learn both fine details and broad structures, improving accuracy and robustness.
                </p>
              </td>
            </div>

            <div class="columns is-centered has-text-justified">
              <td colspan="3">
                <img src="images/method6_00.png" alt="" width="700" />
              </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                 <p>
                  <b>{Overview of Cross-Context Distillation</b>: Our method combines local and global depth information to enhance the student model’s predictions. It includes two scenarios: (1) \textit{Shared-Context Distillation}, where both models use the same image for distillation; and (2) \textit{Local-Global Distillation}, where the teacher predicts depth for overlapping patches while the student predicts the full image. The Local-Global loss $\mathcal{L}_{\text{lg}}$ (Top Right) ensures consistency between local and global predictions, enabling the student to learn both fine details and broad structures, improving accuracy and robustness.
                </p>
              </td>
            </div>
            <p><br></p>
            <div class="columns is-centered has-text-justified">
            <td colspan="3">
                <img src="images/norm5_00.png" alt="" width="700" />
            </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                   <b>Normalization Strategies.</b>: We compare four normalization strategies: Global Norm~\cite{ranftl2020midas}, Hybrid Norm~\cite{zhang2022hdn}, Local Norm, and No Norm. The figure visualizes how each strategy processes pixels within the normalization region (Norm. Area). The red dot represents any pixel within the region.
                </p>
              </td>
            </div>
            <p><br></p>

            <div class="columns is-centered has-text-justified">
            <td colspan="4">
                <img src="images/context-distll1_00.png alt="" width="300" />
            </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                    <b>Different Inputs Lead to Different Pseudo Labels</b>: Global Depth: The teacher model predicts depth using the entire image, and the local region's prediction is cropped from the output. Local Depth: The teacher model directly takes the cropped local region as input, resulting in more refined and detailed depth estimates for that area, capturing finer details compared to using the entire image.
                </p>
              </td>
            </div>
            <div class="columns is-centered has-text-justified">
            <td colspan="4">
                <img src="images/multi-teacher1_00.png alt="" width="300" />
            </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                    <b>{Multi-teacher Mechanism.</b>: We introduce a multi-teacher distillation approach, where pseudo-labels are generated from multiple teacher models. At each training iteration, one teacher is randomly selected to produce pseudo-labels for unlabeled images.
                </p>
              </td>
            </div>
          </div>
  </section>
  <!--End paper poster -->


  <!-- BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{yan2024distillanydepth,
        title={Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator}, 
        author={Xiankang He, Dongyan Guo, Hongji Li, Ruibo Li, Ying Cui, and Chi Zhang},
        year={2024},
        eprint={2412.08503},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2412.08503}, 
      }</code></pre>
    </div>
  </section>
  <!--End BibTeX citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Website source code based on the <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
