<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator">
  <meta name="keywords" content="Monocular Depth Estimation, Distillation, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/shuiyued" target="_blank">Xiankang He</a><sup>1*,2</sup>,
              </span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Dongyan Guo</a><sup>1*</sup>,
                </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hongji Li</a><sup>2,3</sup>,
                  </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ruibo Li</a><sup>4</sup>,
                  </span>
                <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ying Cui</a><sup>1</sup>,
                  </span>
              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Chi Zhang</a><sup>2✉</sup>
                  </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ZJUT&emsp;&emsp;&emsp;<sup>2</sup>WestLake University&emsp;&emsp;&emsp;<sup>3</sup>LZU&emsp;&emsp;&emsp;<sup>4</sup>NTU</span>
              <span class="eql-cntrb"><small><br><sup>✉</sup>Indicates corresponding author</small></span>
              <span class="visit"><small><br><sup>*</sup>This work was done while Xiankang He was visiting WestLake University.</small></span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.08503"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"> 
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/Westlake-AGI-Lab/Distill-Any-Depth" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Demo link -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/xingyang1/Distill-Any-Depth" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf.png" alt="Hugging Face Demo">
                    </span>
                    <span>Demo</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="images/teaser8_00.png" alt=""/>

        <h2 class="content has-text-centered">
            <strong>Zero-shot prediction on in-the-wild images.</strong> Our model, distilled from Genpercept and DepthAnythingv2, outperforms other methods by delivering more accurate depth details and exhibiting superior generalization for monocular depth estimation on in-the-wild images. 
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->

  
  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-2">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Monocular depth estimation (MDE) aims to predict scene depth from a single RGB image and plays a crucial role in 3D scene understanding. Recent advances in zero-shot MDE leverage normalized depth representations and distillation-based learning to improve generalization across diverse scenes. However, current depth normalization methods for distillation, relying on global normalization, can amplify noisy pseudo-labels, reducing distillation effectiveness. In this paper, we systematically analyze the impact of different depth normalization strategies on pseudo-label distillation. Based on our findings, we propose Cross-Context Distillation, which integrates global and local depth cues to enhance pseudo-label quality. Additionally, we introduce a multi-teacher distillation framework that leverages complementary strengths of different depth estimation models, leading to more robust and accurate depth predictions. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, both quantitatively and qualitatively.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->
  

  <!-- Paper poster -->
  <section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-">
            <h2 class="title is-2">Method</h2>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                  we introduce a novel distillation framework designed to leverage unlabeled images for training zero-shot Monocular Depth Estimation (MDE) models. 
                </p>
              </td>
            </div>

            <div class="columns is-centered has-text-justified">
              <td colspan="3">
                <img src="images/method6_00.png" alt="" width="700" />
              </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                 <p>
                  <b>Overview of Cross-Context Distillation</b>: Our method combines local and global depth information to enhance the student model’s predictions. It includes two scenarios: (1) Shared-Context Distillation, where both models use the same image for distillation; and (2) Local-Global Distillation, where the teacher predicts depth for overlapping patches while the student predicts the full image. The Local-Global loss (Top Right) ensures consistency between local and global predictions, enabling the student to learn both fine details and broad structures, improving accuracy and robustness.
                </p>
              </td>
            </div>
            <p><br></p>
            <div class="columns is-centered has-text-justified">
            <td colspan="3">
                <img src="images/norm5_00.png" alt="" width="400" />
            </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                   <b>Normalization Strategies</b>: We compare four normalization strategies: Global Norm, Hybrid Norm, Local Norm, and No Norm. The figure visualizes how each strategy processes pixels within the normalization region (Norm. Area). The red dot represents any pixel within the region.
                </p>
              </td>
            </div>
            <p><br></p>

            <div class="columns is-centered has-text-justified">
            <td colspan="4">
                <img src="images/context-distll1_00.png" alt="" width="400" />
            </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                    <b>Different Inputs Lead to Different Pseudo Labels</b>: Global Depth: The teacher model predicts depth using the entire image, and the local region's prediction is cropped from the output. Local Depth: The teacher model directly takes the cropped local region as input, resulting in more refined and detailed depth estimates for that area, capturing finer details compared to using the entire image.
                </p>
              </td>
            </div>
            <div class="columns is-centered has-text-justified">
            <td colspan="4">
                <img src="images/multi-teacher1_00.png" alt="" width="400" />
            </td>
            </div>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                    <b>Multi-teacher Mechanism</b>: We introduce a multi-teacher distillation approach, where pseudo-labels are generated from multiple teacher models. At each training iteration, one teacher is randomly selected to produce pseudo-labels for unlabeled images.
                </p>
              </td>
            </div>
          </div>
  </section>
  <!--End paper poster -->

  <section class="section hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column custom-width">
            <h2 class="title is-2">Results</h2>
            <div class="content has-text-justified">
              <td colspan="3">
                <p>
                  Our model achieves SOTA performance across both indoor and outdoor scenes.
                </p>
              </td>
            </div>
            <h3 class="title is-3">Qualitative Comparison of Relative Depth Estimations</h3>
            <div class="content has-text-justified">
              <td colspan="3">
                <img src="images/vis_comp_00.png" alt="" width="2160" />
                <p>
                  We present visual comparisons of depth predictions from our method ("Ours") alongside other classic depth estimators ("MiDaS v3.1", and models using DINOv2 or SD as priors ("DepthAnythingv2", "Marigold", "Genpercept"). Compared to state-of-the-art methods, the depth map produced by our model, particularly at the position indicated by the \textbf{black arrow}, exhibits finer granularity and more detailed depth estimation.
                </p>
              </td>
            </div>

            <h3 class="title is-3">Qualitative Comparison with Baseline Distillation</h3>
            <div class="content has-text-justified">
              <td colspan="3">
                <img src="images/depth_point2_00.png" alt="" width="2160" />
                <p>
                  We compare our method with the baseline as the previous distillation method, which uses only global normalization. The red diagonal lines represent the ground truth, with results closer to the lines indicating better performance. Our method produces smoother surfaces, sharper edges, and more detailed depth maps.
                </p>
              </td>
            </div>

            <h3 class="title is-3">Ablation Study</h3>
            <div class="content has-text-justified" style="text-align: center;">
              <img src="images/cvpr25_02.png" alt="" width="2160" />
              <p>
                For the ablation study and analysis, we sample a subset of 50K images from SA-1B as our training data, with an input image size of 560 × 560 for the network. We conduct experiments on two of the most challenging benchmarks, DIODE and ETH3D, which include both indoor and outdoor scenes. The model was trained with a batch size of 4 and converged after approximately 20,000 iterations on a single NVIDIA V100 GPU.
              </p>
            </div>
            
            <h3 class="title is-3">More comparisons with State-of-the-Arts</h3>
            <div class="content has-text-justified" style="text-align: center;">
              <img src="images/cvpr25_03.png" alt="" width="2160" />
              <p>
                Our model achieves SOTA performance across both indoor and outdoor datasets, demonstrating strong generalization from structured indoor scenes (NYUv2, ScanNet) to complex outdoor environments (KITTI, DIODE, ETH3D), as shown in Table 5. By optimizing pseudo-label distillation and depth normalization, our student model not only surpasses its teacher but also achieves a new SOTA on multiple benchmarks, demonstrating the effectiveness of our approach.
              </p>
            </div>
            
            <h3 class="title is-3">Distilled Generative Models</h3>
            <div class="columns is-centered has-text-centered">
              <td colspan="3">
                <img src="images/distill_gen_00.png" alt="" width="2160" />
              </td>
            </div>
            <div class="content has-text-justified">
            <td colspan="3">
              <p>
                Instead of just distilling classical depth models, we also apply distillation to generative models, aiming for the student model to capture their rich details.
              </p>
            </td>
            </div>
            
            <h3 class="title is-3">Additional Results on Depth Estimation in the Wild</h3>
            <div class="columns is-centered has-text-centered">
              <td colspan="3">
                <img src="images/depthmap.png" alt="" width="2160" />
              </td>
            </div>
            <div class="content has-text-justified">
            <td colspan="3">
              <p>
                We showcase more depth maps generated by our model on in-the-wild scenes, highlighting its robustness and precision.
              </p>
            </td>
            </div>

            <h3 class="title is-3">Additional Results on Depth Estimation in the Wild</h3>
            <div class="columns is-centered has-text-centered">
              <td colspan="3">
                <img src="images/depthmap.png" alt="" width="2160" />
              </td>
            </div>
            <div class="content has-text-justified">
            <td colspan="3">
              <p>
                We showcase more depth maps generated by our model on in-the-wild scenes, highlighting its robustness and precision.
              </p>
            </td>
            </div>


<!--             <h3 class="title is-3">Teacher model integration with StyleCrafter</h3>
            <div class="content has-text-justified">
              <td colspan="3">
                <img src="assets/stylecrafter_integration.jpg" alt="" width="2160" />
                <p>
                  Impact of Teacher Model on StyleCrafter Image Generation. The term "timestep" refers to the number of denoising steps during which the Teacher Model is involved.
                  In addition to ensuring layout stability, the Teacher Model also effectively reduces the occurrence of content leakage when applied to StyleCrafter.
                </p>
              </td>
            </div> -->

          </div>
        </div>
        <p><br></p>
      </div>
    </div>
  </section>

  <!-- BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{yan2024distillanydepth,
        title={Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator}, 
        author={Xiankang He, Dongyan Guo, Hongji Li, Ruibo Li, Ying Cui, and Chi Zhang},
        year={2024},
        eprint={2412.08503},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2412.08503}, 
      }</code></pre>
    </div>
  </section>
  <!--End BibTeX citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Website source code based on the <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
